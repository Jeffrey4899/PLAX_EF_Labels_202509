{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d4cb641-8d9e-4010-8b73-c1ae4698a601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e343c38-b8c4-4f8d-926d-969ab98113ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set option to display more columns\n",
    "pd.set_option('display.max_columns', None)  # None means unlimited\n",
    "pd.set_option('display.max_rows', None)  \n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef8573de-c64a-49b0-aeac-ad9eb6fb0ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_echo = pd.read_csv(\"20240417_all_60days_closest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33a766c0-995a-4f4e-a314-9750c65874f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2560"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_echo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aa1e665-11d0-4b07-8f20-4090789e0da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['note_id_x', 'subject_id', 'hadm_id', 'note_type', 'note_seq_x',\n",
       "       'charttime', 'storetime', 'text', 'study_id', 'study_datetime',\n",
       "       'note_id_y', 'note_seq_y', 'note_charttime', 'time_difference'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_echo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03a770e0-1b4d-4f86-b895-1c609852b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "note_ids = [\n",
    "    '12211141-DS-20',\n",
    "    '12466651-DS-20',\n",
    "    '16086976-DS-10',\n",
    "    '10966093-DS-22',\n",
    "    '11994782-DS-2',\n",
    "    '13860987-DS-16',\n",
    "    '17055745-DS-4',\n",
    "    '18001424-DS-16',\n",
    "    '18436961-DS-24',\n",
    "    '19275910-DS-7',\n",
    "    '17121541-DS-18',\n",
    "    '17618022-DS-25',\n",
    "    '10239721-DS-13',\n",
    "    '12137011-DS-32',\n",
    "    '12323804-DS-16',\n",
    "    '16240920-DS-16',\n",
    "    '16009371-DS-19',\n",
    "    '19242994-DS-19',\n",
    "    '10367718-DS-25',\n",
    "    '12660752-DS-11',\n",
    "    '13297743-DS-111',\n",
    "    '13707062-DS-17',\n",
    "    '14290075-DS-30',\n",
    "    '15461582-DS-27',\n",
    "    '16169603-DS-19',\n",
    "    '17673024-DS-7',\n",
    "    '17897160-DS-10',\n",
    "    '19066200-DS-17',\n",
    "    '19408205-DS-18',\n",
    "    '11622732-DS-3',\n",
    "    '10629866-DS-12',\n",
    "    '11006886-DS-20'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "389e6b31-056c-4711-9c50-4c0106f741f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tricky_ones = all_echo[all_echo['note_id_x'].isin(note_ids)]\n",
    "tricky_ones.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00c2df44-b24c-453c-ab2d-376beac1e9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_columns(df):\n",
    "    # Define all possible column labels from the sample outputs\n",
    "    column_labels = [\n",
    "        \"Ejection Fraction (EF)\", \"tokens used\"\n",
    "    ]\n",
    "    \n",
    "    # Append missing columns to echo_df\n",
    "    for column in column_labels:\n",
    "        if column not in df.columns:\n",
    "            df[column] = None  # Initialize new columns with None values\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0eb80d5-8fa5-48bf-a6ea-0e5b446d0e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPT_extract_data_prev(i, df):\n",
    "    completion = client.chat.completions.create(\n",
    "      model=\"gpt-4-turbo\",\n",
    "      # model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \n",
    "                \"content\":\n",
    "                    \"You are an experienced Machine Learning Engineer, Data Scientist and an experienced Cardiologist. \\n\"\n",
    "                    \"Your goal is to extract target data (Left Ventricle Ejection Fraction or EF) from the given cardiology note. \\n\"\n",
    "                    \"The study of interest is only TTE. EF values from TEE, Nuclear, Cardiac MRC, etc should not be considered. \\n\"\n",
    "                    \"Your output should contain nothing other than the data. \\n\"\n",
    "                    \"Your output should be an integer. If no such data available, put 'NAN' to the value. \\n\"\n",
    "                    \"Your output should contain nothing other than the data itself.\\n\"\n",
    "                    \"You can add parentheses after the data for a very brief explanation if you are not certain.\\n\"\n",
    "                    \"For example, if the EF mentioned in the note is a range, you should take an average of that range, then put the range in the parentheses.\\n\"\n",
    "                    \"Sample output: 60 (55-65) \\n\"\n",
    "                    \"For another example, if there are different TTE studies and you are not sure which EF to use, do a best guess and specify that situation in the parentheses.\\n\"\n",
    "                    \"Sample output: 55 (multiple studies, took the value from the 1st one) \\n\"\n",
    "                    \"Be very careful not to take the EF values from the patient's medical history.\"\n",
    "            },\n",
    "        {\"role\": \"user\", \"content\": df.iloc[i,7]}\n",
    "      ]\n",
    "    )\n",
    "    return completion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e95b3c27-cca7-448e-a15a-65b98e19c14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPT_extract_data(i, df):\n",
    "    completion = client.chat.completions.create(\n",
    "      model=\"gpt-4-turbo\",\n",
    "      # model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \n",
    "                \"content\":\n",
    "                    \"You are an experienced Data Scientist and Cardiologist. \\n\"\n",
    "                    \"Your goal is to extract the Left Ventricle Ejection Fraction (also known as LVEF or EF) from doctors' notes about a patient during a hospital stay. \\n\"\n",
    "                    \"The given note will contain a lot of irrelevant information that is not the EF, which should be ignored. \\n\"\n",
    "                    \"You should only report EF values from a transthoracic echocardiogram study, also known as an Echo or TTE. \\n\"\n",
    "                    \"TTE results will often (but not always) appear under a heading like 'Pertinent Results', 'Pertinent Studies', or 'Imaging Reports'. \\n\"\n",
    "                    \"EF values from transesophageal, TEE, Nuclear, MRI, cardiac perfusion, x-ray, ct, PUMP, bedside ultrasound, or any other non-TTE study types should not be considered. \\n\"\n",
    "                    \"Do not report any EF values recorded as part of the patient's medical history. Only record values from a TTE study that took place during this hospital stay. \\n\"\n",
    "                    \"Notes will often have sections titled 'Brief Hospital Course' and 'History of Present Illness'. EF values reported early in these sections where it talks about patient history or what the patient presented with should be ignored. \\n\"\n",
    "                    \"The EF in the note may be given as an integer value (e.g. 55%), as a range (e.g. 55-60%), or as an inequality (e.g. >55%). \\n\"\n",
    "                    \"Ignore qualitative notes about the EF such as 'low normal', 'recovered', 'severely depressed', 'elevated', etc. Only look at numerical values. \\n\"\n",
    "                    \"Output format: \\n\"\n",
    "                    \"1. Your output should only contain the desired data, and nothing other than that.\\n\"\n",
    "                    \"2. If there is only one TTE/Echo study in the note with a valid EF value, report just the integer value, the range or the inequality. \\n\"\n",
    "                    \"   Sample outputs: '55'; '55-60'; '>55'.\\n\"\n",
    "                    \"3. If there is no valid EF data in the note or the only EF values reported are from medical history, just report 'NAN'. \\n\"\n",
    "                    \"4. If two or more TTE/Echo studies(not from medical history) were performed on the patient, report 'Multiple TTE/Echo Studies (x/y):' followed by EF values separates by commas. \\n\"\n",
    "                    \"   x >= 1 indicates the number of TTE/Echo studies that give a EF value, y >= 2 indicates how many TTE/Echo studies are there.\\n\"\n",
    "                    \"   You should output in this format only when there are two or more TTE/Echo studies and have one or more EF values given in the note. \\n\"\n",
    "                    \"   If there is only one TTE/Echo study, you should follow one of the output formats as specified in (2), (3) or (5). \\n\"\n",
    "                    \"   If there are multiple studies but no specific EF value given, report 'NAN'.\\n\"\n",
    "                    \"   Sample outputs: 'Multiple TTE/Echo Studies(1/2): 55, _'; 'Multiple TTE/Echo Studies(2/3): 25, _, 55'; 'Multiple TTE/Echo Studies(2/2): 40, 55'. \\n\"\n",
    "                    \"   This does not apply if the same EF is just reported multiple times; for example, if there's only one TTE/Echo study and it is listed in one place as '20%' and in another as 'EF 20%', just report '20'. \\n\"\n",
    "                    \"   Some EF values are redacted and reported something like 'LVEF = ___ %'. Ignore these redacted values, but still report if there are multiple different TTTE/EchoTE studies even if some have redacted EF values. \\n\"\n",
    "                    \"5. Some notes may not be clear and can be confusing. If you are uncertain about the correct answer, you should report: 'Uncertain(Reason/Explanation) ' followed by your best guess.\\n\"\n",
    "                    \"   Sample outputs: 'Uncertain(Not sure if data is from history): 55'; 'Uncertain(Not sure if there's multiple TTE/Echo studies): 66'.\\n\"\n",
    "            },\n",
    "        {\"role\": \"user\", \"content\": df.iloc[i,7]}\n",
    "      ]\n",
    "    )\n",
    "    return completion\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67e3438a-f949-4705-b1a9-d3995b5a2a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLP_notes(df_ori, output_name):\n",
    "    num_rows = df_ori.shape[0]\n",
    "    df = df_ori.copy()\n",
    "    \n",
    "    for i in range(num_rows):    \n",
    "        completion = GPT_extract_data(i, df_ori)  # Assuming this function returns the completion object\n",
    "        total_tokens = completion.usage.total_tokens  # Extracting the total_tokens value\n",
    "        update_row_from_completion(df, i, completion, total_tokens)\n",
    "        if i % 10 == 0:\n",
    "            print(i)\n",
    "        if i % 100 == 0:\n",
    "            df.to_csv(\"temp_saving.csv\", index=False) \n",
    "\n",
    "    processed_filename = f\"{output_name}_processed.csv\"\n",
    "    note_dropped_filename = f\"{output_name}_processed_note_dropped.csv\"\n",
    "    \n",
    "    df.to_csv(processed_filename, index=False)\n",
    "    print(f\"Saved processed file as: {processed_filename}\")\n",
    "    \n",
    "    df_drop_note = df.copy()\n",
    "    df_drop_note = df_drop_note.drop(columns=['text'])\n",
    "    df_drop_note.to_csv(note_dropped_filename, index=False)\n",
    "    print(f\"Saved note-dropped file as: {note_dropped_filename}\")\n",
    "    return df_drop_note\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310a2d59-2248-4cf6-90c2-cd7ae1a8831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_row_from_completion(df, i, completion, total_tokens):\n",
    "    completion_text = completion.choices[0].message.content  # Extracting the text part of the completion message\n",
    "    lines = completion_text.split('\\n')\n",
    "    for line in lines:\n",
    "        if ': ' in line:  # Ensure the line contains a key-value pair\n",
    "            key, value = line.split(': ', 1)\n",
    "            value = value.strip()\n",
    "            # Replace 'NAN' text with np.nan\n",
    "            if value == 'NAN':\n",
    "                value = np.nan\n",
    "            # Check if the key exists as a column in the DataFrame; if it does, update the value\n",
    "            if key in df.columns:\n",
    "                df.at[i, key] = value\n",
    "            else:\n",
    "                print(f\"Column {key} does not exist in the DataFrame.\")\n",
    "                \n",
    "    df.at[i, 'tokens used'] = total_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3d7215b-7344-40ae-9764-8744f5d76b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.5 (75-100)\n",
      "6107\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)\n",
    "print(completion.usage.total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84816854-da77-4308-bf2d-e29ad3c10bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_echo = append_columns(all_echo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04c39814-f190-43e8-9b1e-b4e98d3983f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n",
      "630\n",
      "640\n",
      "650\n",
      "660\n",
      "670\n",
      "680\n",
      "690\n",
      "700\n",
      "710\n",
      "720\n",
      "730\n",
      "740\n",
      "750\n",
      "760\n",
      "770\n",
      "780\n",
      "790\n",
      "800\n",
      "810\n",
      "820\n",
      "830\n",
      "840\n",
      "850\n",
      "860\n",
      "870\n",
      "880\n",
      "890\n",
      "900\n",
      "910\n",
      "920\n",
      "930\n",
      "940\n",
      "950\n",
      "960\n",
      "970\n",
      "980\n",
      "990\n",
      "1000\n",
      "1010\n",
      "1020\n",
      "1030\n",
      "1040\n",
      "1050\n",
      "1060\n",
      "1070\n",
      "1080\n",
      "1090\n",
      "1100\n",
      "1110\n",
      "1120\n",
      "1130\n",
      "1140\n",
      "1150\n",
      "1160\n",
      "1170\n",
      "1180\n",
      "1190\n",
      "1200\n",
      "1210\n",
      "1220\n",
      "1230\n",
      "1240\n",
      "1250\n",
      "1260\n",
      "1270\n",
      "1280\n",
      "1290\n",
      "1300\n",
      "1310\n",
      "1320\n",
      "1330\n",
      "1340\n",
      "1350\n",
      "1360\n",
      "1370\n",
      "1380\n",
      "1390\n",
      "1400\n",
      "1410\n",
      "1420\n",
      "1430\n",
      "1440\n",
      "1450\n",
      "1460\n",
      "1470\n",
      "1480\n",
      "1490\n",
      "1500\n",
      "1510\n",
      "1520\n",
      "1530\n",
      "1540\n",
      "1550\n",
      "1560\n",
      "1570\n",
      "1580\n",
      "1590\n",
      "1600\n",
      "1610\n",
      "1620\n",
      "1630\n",
      "1640\n",
      "1650\n",
      "1660\n",
      "1670\n",
      "1680\n",
      "1690\n",
      "1700\n",
      "1710\n",
      "1720\n",
      "1730\n",
      "1740\n",
      "1750\n",
      "1760\n",
      "1770\n",
      "1780\n",
      "1790\n",
      "1800\n",
      "1810\n",
      "1820\n",
      "1830\n",
      "1840\n",
      "1850\n",
      "1860\n",
      "1870\n",
      "1880\n",
      "1890\n",
      "1900\n",
      "1910\n",
      "1920\n",
      "1930\n",
      "1940\n",
      "1950\n",
      "1960\n",
      "1970\n",
      "1980\n",
      "1990\n",
      "2000\n",
      "2010\n",
      "2020\n",
      "2030\n",
      "2040\n",
      "2050\n",
      "2060\n",
      "2070\n",
      "2080\n",
      "2090\n",
      "2100\n",
      "2110\n",
      "2120\n",
      "2130\n",
      "2140\n",
      "2150\n",
      "2160\n",
      "2170\n",
      "2180\n",
      "2190\n",
      "2200\n",
      "2210\n",
      "2220\n",
      "2230\n",
      "2240\n",
      "2250\n",
      "2260\n",
      "2270\n",
      "2280\n",
      "2290\n",
      "2300\n",
      "2310\n",
      "2320\n",
      "2330\n",
      "2340\n",
      "2350\n",
      "2360\n",
      "2370\n",
      "2380\n",
      "2390\n",
      "2400\n",
      "2410\n",
      "2420\n",
      "2430\n",
      "2440\n",
      "2450\n",
      "2460\n",
      "2470\n",
      "2480\n",
      "2490\n",
      "2500\n",
      "2510\n",
      "2520\n",
      "2530\n",
      "2540\n",
      "2550\n",
      "Saved processed file as: 20240424_GPT4_turbo_EF_only_v2_processed.csv\n",
      "Saved note-dropped file as: 20240424_GPT4_turbo_EF_only_v2_processed_note_dropped.csv\n"
     ]
    }
   ],
   "source": [
    "output_name = '20240424_GPT4_turbo_EF_only_v2'\n",
    "df_temp = all_echo\n",
    "num_rows = df_temp.shape[0]\n",
    "df = df_temp.copy()\n",
    "\n",
    "for i in range(num_rows):    \n",
    "    completion = GPT_extract_data(i, df_temp)  # Assuming this function returns the completion object\n",
    "    completion_text = completion.choices[0].message.content\n",
    "    total_tokens = completion.usage.total_tokens  # Extracting the total_tokens value\n",
    "    df.at[i, 'Ejection Fraction (EF)'] = completion_text\n",
    "    df.at[i, 'tokens used'] = total_tokens\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "    if i % 100 == 0:\n",
    "       df.to_csv(\"temp_saving.csv\", index=False) \n",
    "\n",
    "processed_filename = f\"{output_name}_processed.csv\"\n",
    "note_dropped_filename = f\"{output_name}_processed_note_dropped.csv\"\n",
    "\n",
    "df.to_csv(processed_filename, index=False)\n",
    "print(f\"Saved processed file as: {processed_filename}\")\n",
    "\n",
    "df_drop_note = df.copy()\n",
    "df_drop_note = df_drop_note.drop(columns=['text'])\n",
    "df_drop_note.to_csv(note_dropped_filename, index=False)\n",
    "print(f\"Saved note-dropped file as: {note_dropped_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "891fd3c5-5f45-4185-aa4f-707b97f5f45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gzy\\AppData\\Local\\Temp\\ipykernel_33928\\2271959150.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = None  # Initialize new columns with None values\n",
      "C:\\Users\\gzy\\AppData\\Local\\Temp\\ipykernel_33928\\2271959150.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = None  # Initialize new columns with None values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'note_dropped_filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 27\u001b[0m\n\u001b[0;32m     16\u001b[0m        df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp_saving.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# processed_filename = f\"{output_name}_processed.csv\"\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# note_dropped_filename = f\"{output_name}_processed_note_dropped.csv\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# df_drop_note = df_drop_note.drop(columns=['text'])\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# df_drop_note.to_csv(note_dropped_filename, index=False)\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved note-dropped file as: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnote_dropped_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'note_dropped_filename' is not defined"
     ]
    }
   ],
   "source": [
    "tricky_ones = append_columns(tricky_ones)\n",
    "output_name = '20240420_GPT4_turbo_tricky_EF_only_v2.3'\n",
    "df_temp = tricky_ones\n",
    "num_rows = df_temp.shape[0]\n",
    "df = df_temp.copy()\n",
    "\n",
    "for i in range(num_rows):    \n",
    "    completion = GPT_extract_data(i, df_temp)  # Assuming this function returns the completion object\n",
    "    completion_text = completion.choices[0].message.content\n",
    "    total_tokens = completion.usage.total_tokens  # Extracting the total_tokens value\n",
    "    df.at[i, 'Ejection Fraction (EF)'] = completion_text\n",
    "    df.at[i, 'tokens used'] = total_tokens\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "    if i % 100 == 0:\n",
    "       df.to_csv(\"temp_saving.csv\", index=False) \n",
    "\n",
    "# processed_filename = f\"{output_name}_processed.csv\"\n",
    "# note_dropped_filename = f\"{output_name}_processed_note_dropped.csv\"\n",
    "\n",
    "# df.to_csv(processed_filename, index=False)\n",
    "# print(f\"Saved processed file as: {processed_filename}\")\n",
    "\n",
    "# df_drop_note = df.copy()\n",
    "# df_drop_note = df_drop_note.drop(columns=['text'])\n",
    "# df_drop_note.to_csv(note_dropped_filename, index=False)\n",
    "print(f\"Saved note-dropped file as: {note_dropped_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320e4df5-2130-46ba-a46b-a36c8aa3c555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2145e97-fada-4304-9445-147ff410414d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7122498-3742-47d6-a6f0-32e04047d49f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c68ff2-beb7-42ce-8833-07bd748790e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373932ad-ee3f-4f76-b3ea-fd567d8ca42d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf468551-eeae-4eab-95fd-d6a169fad39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "  name=\"Cardiologist\",\n",
    "  instructions=                    \n",
    "    \"You are an experienced Cardiologist. \\n\"\n",
    "    \"Your goal is to extract target data (Left Ventricle Ejection Fraction or EF) from the given cardiology note. \\n\"\n",
    "    \"The study of interest is only TTE. EF values from TEE, Nuclear, Cardiac MRC, etc should not be considered. \\n\"\n",
    "    \"Your output should contain nothing other than the data. \\n\"\n",
    "    \"Your output should be an integer. If no such data available, put 'NAN' to the value. \\n\"\n",
    "    \"Your output should contain nothing other than the data itself.\\n\"\n",
    "    \"You can add parentheses after the data for a very brief explanation if you are not certain.\\n\"\n",
    "    \"For example, if the EF mentioned in the note is a range, you should take an average of that range, then put the range in the parentheses.\\n\"\n",
    "    \"Sample output: 60 (55-65) \\n\"\n",
    "    \"For another example, if there are different TTE studies and you are not sure which EF to use, do a best guess and specify that situation in the parentheses.\\n\"\n",
    "    \"Sample output: 55 (multiple studies, took the value from the 1st one) \\n\"\n",
    "    \"Be very careful not to take the EF values from the patient's medical history.\",\n",
    "  model=\"gpt-4-turbo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6625420-c57b-4e5b-ae16-c8ffcd4c3a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = client.beta.threads.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ae051a6-3b56-4171-a4b2-26c70bdbfac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = client.beta.threads.messages.create(\n",
    "  thread_id=thread.id,\n",
    "  role=\"user\",\n",
    "  content=tricky_ones.iloc[0,7]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc18fbce-b4ae-4790-b77d-c127ea93f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.beta.threads.runs.create(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f1aadb-2f01-417f-b1fe-6f8a80fabd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    if run.status == 'completed': \n",
    "        messages = client.beta.threads.messages.list(\n",
    "            thread_id=thread.id\n",
    "        )\n",
    "        print(messages)\n",
    "        break\n",
    "    else:\n",
    "        print(run.status)\n",
    "        time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
